{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import gym_tictactoe as t3\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "n_input = (3 * 3 * 3) * 3\n",
    "n_hidden = 1000\n",
    "n_output = 3 * 3 * 3\n",
    "\n",
    "checkpoint_path = './my_tictactoe.ckpt'\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = gym.make('tictactoe-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(np.array([[0], [1], [2]]))\n",
    "\n",
    "def convert_game_to_x_state(obs):\n",
    "    # gym_tictactoe now supports int-encoded world\n",
    "    world = np.array(obs, dtype=np.float32)\n",
    "    data = list(map(lambda x: [x], world.flatten()))\n",
    "    return encoder.transform(data).flatten()\n",
    "\n",
    "def convert_action_to_step(action, player):\n",
    "    action = int(action)\n",
    "    val = 0\n",
    "    multiplier = 1\n",
    "    while action:\n",
    "        val += (action%3)*multiplier\n",
    "        multiplier *= 10\n",
    "        action //= 3\n",
    "    \n",
    "    return str(player) + str(val).zfill(3)\n",
    "\n",
    "def discount_rewards(rewards, discount_rate):\n",
    "    reward_epsilon = 0.1\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = (rewards[step] + reward_epsilon) + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate=discount_rate) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "player_scopes = ['player_1', 'player_2']\n",
    "all_logits = []\n",
    "outputs = []\n",
    "ys = []\n",
    "cross_entropies = []\n",
    "training_ops = []\n",
    "all_gradient_placeholders = []\n",
    "all_gradients = []\n",
    "optimizers = []\n",
    "\n",
    "X_state = tf.placeholder(shape=(None, n_input), dtype=tf.float32, name='X')\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "for scope in player_scopes:\n",
    "    with tf.variable_scope(scope):\n",
    "        hidden = fully_connected(X_state, n_hidden, activation_fn=tf.nn.relu, weights_initializer=initializer)\n",
    "        logits = fully_connected(hidden, n_output, activation_fn=None, weights_initializer=initializer)\n",
    "        all_logits.append(logits)\n",
    "\n",
    "        output = tf.contrib.layers.softmax(logits)\n",
    "        outputs.append(output)\n",
    "\n",
    "        y = tf.to_float(tf.multinomial(tf.log(output), num_samples=n_output))\n",
    "        ys.append(y)\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "        cross_entropies.append(cross_entropy)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizers.append(optimizer)\n",
    "\n",
    "        trainable_variables = tf.trainable_variables(scope=scope)\n",
    "        grads_and_vars = optimizer.compute_gradients(cross_entropy, trainable_variables)\n",
    "        gradients = [grad for grad, variable in grads_and_vars]\n",
    "        all_gradients.append(gradients)\n",
    "        gradient_placeholders = []\n",
    "        grads_and_vars_feed = []\n",
    "\n",
    "        for grad, variable in grads_and_vars:\n",
    "            gradient_placeholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "            gradient_placeholders.append(gradient_placeholder)\n",
    "            grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "        training_op = optimizer.apply_gradients(grads_and_vars_feed, global_step=global_step)\n",
    "        training_ops.append(training_op)\n",
    "        all_gradient_placeholders.append(gradient_placeholders)\n",
    "\n",
    "        \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter('logs', tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23,15,2,20,7,7, = -1\n",
      "13,26,21,25,6,9,8,24, = 1\n",
      "20,17,24,3,18,11,3, = -1\n",
      "26,2,13,13, = -1\n",
      "24,14,5,16,17,17, = -1\n",
      "17,0,3,24,14,17, = -1\n",
      "16,25,26,25, = -1\n",
      "8,21,26,18,18, = -1\n",
      "23,21,14,7,23, = -1\n",
      "17,5,11,25,19,17, = -1\n",
      "Game rewards (0.0):-1\n",
      "18,7,2,2, = -1\n",
      "21,9,10,1,13,10, = -1\n",
      "12,3,12, = -1\n",
      "1,21,7,21, = -1\n",
      "21,20,20, = -1\n",
      "0,1,4,8,11,11, = -1\n",
      "21,7,22,20,2,3,25,7, = -1\n",
      "3,23,23, = -1\n",
      "7,1,11,8,15,25,23,18,12,12, = -1\n",
      "3,10,7,18,20,12,9,20, = -1\n",
      "Game rewards (0.0):-1\n"
     ]
    }
   ],
   "source": [
    "from timeit import Timer\n",
    "\n",
    "timer = Timer()\n",
    "\n",
    "n_games = 10\n",
    "n_rounds = 2\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for r in range(n_rounds):\n",
    "        gen = r % len(player_scopes)\n",
    "        adv = (r+1) % 2\n",
    "        round_rewards = []\n",
    "        round_gradients = []\n",
    "        start = timer.timer()\n",
    "        for i in range(n_games):\n",
    "            env = gym.make('tictactoe-v0')\n",
    "            obs = env.reset()\n",
    "            turn = 0\n",
    "            done = False\n",
    "            game_rewards = 0\n",
    "            while not done:\n",
    "                player = (turn%2) + 1\n",
    "                world = convert_game_to_x_state(obs)            \n",
    "                with tf.variable_scope(player_scopes[gen]):\n",
    "                    action, grad_result = sess.run([ys[gen], all_gradients[gen]], feed_dict={X_state: [world]})\n",
    "                action_taken = int(action[0][0])\n",
    "                step = convert_action_to_step(action_taken, player)\n",
    "                obs, reward, done, info = env.step(step)\n",
    "                print(action_taken, end=',')\n",
    "                game_rewards += reward\n",
    "                turn += 1\n",
    "                if done:\n",
    "                    break\n",
    "            print(' =', game_rewards)\n",
    "#             game_gradients = gradients[gen].eval(feed_dict=None)\n",
    "            round_rewards.append(game_rewards)\n",
    "            round_gradients.append(grad_result)\n",
    "        print('Game rewards ({}):{}'.format((timer.timer() - start)//1, game_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- x -    x o -    o - -    \n",
      "- - x    o - -    - - -    \n",
      "- - -    - - -    x - -    \n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, gen, adv, gen_starts=True):\n",
    "    # Gen starts the game as player 1\n",
    "    gen_player = 1 if gen_starts else 2\n",
    "    adv_player = 2 if gen_starts else 1\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    action_rewards = []\n",
    "    action_gradients = []\n",
    "    while not done:\n",
    "        if gen_starts:\n",
    "            world = convert_game_to_x_state(obs)\n",
    "            with tf.variable_scope(player_scopes[gen]):\n",
    "                gen_action, gen_grad_result = sess.run([ys[gen], all_gradients[gen]], feed_dict={X_state: [world]})\n",
    "            gen_action_taken = int(gen_action[0][0])\n",
    "            gen_step = convert_action_to_step(gen_action_taken, gen_player)\n",
    "            obs, gen_reward, done, info = env.step(gen_step)\n",
    "            if DEBUG:\n",
    "                print('G({})'.format(gen_step), end=' ')\n",
    "\n",
    "            action_rewards.append(gen_reward)\n",
    "            action_gradients.append(gen_grad_result)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            world = convert_game_to_x_state(obs)\n",
    "            with tf.variable_scope(player_scopes[adv]):\n",
    "                adv_action = sess.run(ys[adv], feed_dict={X_state: [world]})\n",
    "            adv_action_taken = int(adv_action[0][0])\n",
    "            adv_step = convert_action_to_step(adv_action_taken, adv_player)\n",
    "            obs, adv_reward, done, info = env.step(adv_step)\n",
    "\n",
    "            if DEBUG:\n",
    "                print('A({})'.format(adv_step), end=' ')\n",
    "\n",
    "        else:\n",
    "            world = convert_game_to_x_state(obs)\n",
    "            with tf.variable_scope(player_scopes[adv]):\n",
    "                adv_action = sess.run(ys[adv], feed_dict={X_state: [world]})\n",
    "            adv_action_taken = int(adv_action[0][0])\n",
    "            adv_step = convert_action_to_step(adv_action_taken, adv_player)\n",
    "            obs, adv_reward, done, info = env.step(adv_step)\n",
    "\n",
    "            if DEBUG:\n",
    "                print('A({})'.format(adv_step), end=' ')\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            world = convert_game_to_x_state(obs)\n",
    "            with tf.variable_scope(player_scopes[gen]):\n",
    "                gen_action, gen_grad_result = sess.run([ys[gen], all_gradients[gen]], feed_dict={X_state: [world]})\n",
    "            gen_action_taken = int(gen_action[0][0])\n",
    "            gen_step = convert_action_to_step(gen_action_taken, gen_player)\n",
    "            obs, gen_reward, done, info = env.step(gen_step)\n",
    "            if DEBUG:\n",
    "                print('G({})'.format(gen_step), end=' ')\n",
    "\n",
    "            action_rewards.append(gen_reward)\n",
    "            action_gradients.append(gen_grad_result)\n",
    "            \n",
    "    return action_rewards, action_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_tictactoe.ckpt\n",
      "Global step: 82\n",
      "....................................................................................................Global step: 83\n",
      "....................................................................................................Global step: 84\n",
      "....................................................................................................Global step: 85\n",
      "....................................................................................................Global step: 86\n",
      "....................................................................................................Global step: 87\n",
      "....................................................................................................Global step: 88\n",
      "....................................................................................................Global step: 89\n",
      "....................................................................................................Global step: 90\n",
      "....................................................................................................Global step: 91\n",
      "....................................................................................................Global step: 92\n",
      "....................................................................................................Global step: 93\n",
      "....................................................................................................Global step: 94\n",
      "....................................................................................................Global step: 95\n",
      "....................................................................................................Global step: 96\n",
      "....................................................................................................Global step: 97\n",
      "....................................................................................................Global step: 98\n",
      "....................................................................................................Global step: 99\n",
      "....................................................................................................Global step: 100\n",
      "....................................................................................................Global step: 101\n",
      "....................................................................................................Global step: 102\n",
      "....................................................................................................Global step: 103\n",
      "....................................................................................................Global step: 104\n",
      "....................................................................................................Global step: 105\n",
      "....................................................................................................Global step: 106\n",
      "....................................................................................................Global step: 107\n",
      "....................................................................................................Global step: 108\n",
      "....................................................................................................Global step: 109\n",
      "....................................................................................................Global step: 110\n",
      "....................................................................................................Global step: 111\n",
      "....................................................................................................Global step: 112\n",
      "....................................................................................................Global step: 113\n",
      "....................................................................................................Global step: 114\n",
      "....................................................................................................Global step: 115\n",
      "....................................................................................................Global step: 116\n",
      "....................................................................................................Global step: 117\n",
      "....................................................................................................Global step: 118\n",
      "....................................................................................................Global step: 119\n",
      "....................................................................................................Global step: 120\n",
      "....................................................................................................Global step: 121\n",
      "....................................................................................................Global step: 122\n",
      "....................................................................................................Global step: 123\n",
      "....................................................................................................Global step: 124\n",
      "....................................................................................................Global step: 125\n",
      "....................................................................................................Global step: 126\n",
      "....................................................................................................Global step: 127\n",
      "....................................................................................................Global step: 128\n",
      "....................................................................................................Global step: 129\n",
      "....................................................................................................Global step: 130\n",
      "....................................................................................................Global step: 131\n",
      "....................................................................................................Global step: 132\n",
      "....................................................................................................Global step: 133\n",
      "....................................................................................................Global step: 134\n",
      "....................................................................................................Global step: 135\n",
      "....................................................................................................Global step: 136\n",
      "....................................................................................................Global step: 137\n",
      "....................................................................................................Global step: 138\n",
      "....................................................................................................Global step: 139\n",
      "....................................................................................................Global step: 140\n",
      "....................................................................................................Global step: 141\n",
      "....................................................................................................Global step: 142\n",
      "....................................................................................................Global step: 143\n",
      "....................................................................................................Global step: 144\n",
      "....................................................................................................Global step: 145\n",
      "....................................................................................................Global step: 146\n",
      "....................................................................................................Global step: 147\n",
      "....................................................................................................Global step: 148\n",
      "....................................................................................................Global step: 149\n",
      "....................................................................................................Global step: 150\n",
      "....................................................................................................Global step: 151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................Global step: 152\n",
      "....................................................................................................Global step: 153\n",
      "....................................................................................................Global step: 154\n",
      "....................................................................................................Global step: 155\n",
      "....................................................................................................Global step: 156\n",
      "....................................................................................................Global step: 157\n",
      "....................................................................................................Global step: 158\n",
      "....................................................................................................Global step: 159\n",
      "....................................................................................................Global step: 160\n",
      "....................................................................................................Global step: 161\n",
      "....................................................................................................Global step: 162\n",
      "....................................................................................................Global step: 163\n",
      "....................................................................................................Global step: 164\n",
      "....................................................................................................Global step: 165\n",
      "....................................................................................................Global step: 166\n",
      "....................................................................................................Global step: 167\n",
      "....................................................................................................Global step: 168\n",
      "....................................................................................................Global step: 169\n",
      "....................................................................................................Global step: 170\n",
      "....................................................................................................Global step: 171\n",
      "....................................................................................................Global step: 172\n",
      "....................................................................................................Global step: 173\n",
      "....................................................................................................Global step: 174\n",
      "....................................................................................................Global step: 175\n",
      "....................................................................................................Global step: 176\n",
      "....................................................................................................Global step: 177\n",
      "....................................................................................................Global step: 178\n",
      "....................................................................................................Global step: 179\n",
      "....................................................................................................Global step: 180\n",
      "....................................................................................................Global step: 181\n",
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "n_games_per_iteration = 50\n",
    "n_iterations = 100\n",
    "iteration_rewards = []\n",
    "iteration_gradients = []\n",
    "discount_rate = 0.95\n",
    "\n",
    "gen = 0\n",
    "adv = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if path.exists(checkpoint_path + '.meta'):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else: \n",
    "        init.run()\n",
    "        \n",
    "    for it in range(n_iterations):\n",
    "        print('Global step:', global_step.eval())\n",
    "        for g in range(n_games_per_iteration):\n",
    "            game_rewards, game_gradients = play_game(env, gen, adv, False)\n",
    "            if DEBUG:\n",
    "                print('rewards:', game_rewards)\n",
    "\n",
    "            iteration_rewards.append(game_rewards)\n",
    "            iteration_gradients.append(game_gradients)\n",
    "            print('.', end='')\n",
    "            \n",
    "        for g in range(n_games_per_iteration):\n",
    "            game_rewards, game_gradients = play_game(env, gen, adv, True)\n",
    "            if DEBUG:\n",
    "                print('rewards:', game_rewards)\n",
    "\n",
    "            iteration_rewards.append(game_rewards)\n",
    "            iteration_gradients.append(game_gradients)\n",
    "            print('.', end='')\n",
    "            \n",
    "        feed_dict = {}\n",
    "        iteration_rewards = discount_and_normalize_rewards(iteration_rewards, discount_rate)\n",
    "        for var_index, grad_placeholder in enumerate(all_gradient_placeholders[gen]):\n",
    "            mean_gradients = np.mean(\n",
    "                [ reward * iteration_gradients[game_index][step][var_index]\n",
    "                     for game_index, rewards in enumerate(iteration_rewards)\n",
    "                     for step, rewards in enumerate(rewards)],\n",
    "                axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "\n",
    "        with tf.variable_scope(player_scopes[0]):\n",
    "            sess.run(training_ops[gen], feed_dict=feed_dict)\n",
    "        saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_human(env):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    player = 0\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        with tf.variable_scope('player_1'):\n",
    "            while not done:\n",
    "                env.render()\n",
    "                step = input()\n",
    "                obs, reward, done, info = env.step('{}{}'.format(player%2+1, step))\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                player += 1\n",
    "                world = convert_game_to_x_state(obs)\n",
    "                action_result = sess.run(ys[0], feed_dict={X_state: [world]})\n",
    "                action = int(action_result[0][0])\n",
    "                step = convert_action_to_step(action, player%2+1)\n",
    "                obs, reward, done, info = env.step(step)\n",
    "\n",
    "                player += 1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_tictactoe.ckpt\n",
      "- - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "000\n",
      "x - -    - - -    - - -    \n",
      "- - -    - - -    - o -    \n",
      "- - -    - - -    - - -    \n",
      "011\n",
      "x - -    - - -    - - -    \n",
      "- x -    - - -    - o -    \n",
      "- - -    - - -    - - -    \n"
     ]
    }
   ],
   "source": [
    "play_with_human(env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
