{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import gym\n",
    "import gym_tictactoe\n",
    "from timeit import Timer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "n_input = (3 * 3 * 3) * 3\n",
    "n_hidden = 100\n",
    "n_output = 3 * 3 * 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "checkpoint_path = './my_dqn_tictactoe_h{}_lr{}.ckpt'.format(n_hidden, learning_rate)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "env = gym.make('tictactoe-v0')\n",
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(np.array([[0], [1], [2]]))\n",
    "\n",
    "def convert_game_to_x_state(obs):\n",
    "    # gym_tictactoe now supports int-encoded world\n",
    "    world = np.array(obs, dtype=np.float32)\n",
    "    data = list(map(lambda x: [x], world.flatten()))\n",
    "    return encoder.transform(data).flatten()\n",
    "\n",
    "def convert_action_to_step(action, player):\n",
    "    action = int(action)\n",
    "    val = 0\n",
    "    multiplier = 1\n",
    "    while action:\n",
    "        val += (action%3)*multiplier\n",
    "        multiplier *= 10\n",
    "        action //= 3\n",
    "    \n",
    "    return str(player) + str(val).zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_scopes = ['actor', 'critic']\n",
    "all_logits = []\n",
    "outputs = []\n",
    "ys = []\n",
    "all_network_trainable_vars_by_name = []\n",
    "\n",
    "X_state = tf.placeholder(shape=(None, n_input), dtype=tf.float32)\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "for scope in player_scopes:\n",
    "    with tf.variable_scope(scope) as tf_scope:\n",
    "        hidden = fully_connected(X_state, n_hidden, activation_fn=tf.nn.relu, weights_initializer=initializer)\n",
    "        logits = fully_connected(hidden, n_output, activation_fn=None, weights_initializer=initializer)\n",
    "        all_logits.append(logits)\n",
    "\n",
    "        output = tf.contrib.layers.softmax(logits)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        y = tf.to_float(tf.multinomial(tf.log(output), num_samples=n_output))\n",
    "        ys.append(y)\n",
    "        \n",
    "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "        network_trainable_vars_by_name = { var.name[len(tf_scope.name):]: var for var in trainable_vars }\n",
    "        all_network_trainable_vars_by_name.append(network_trainable_vars_by_name)\n",
    "\n",
    "actor_net = outputs[0]\n",
    "critic_net = outputs[1]\n",
    "\n",
    "actor_vars = all_network_trainable_vars_by_name[0]\n",
    "critic_vars = all_network_trainable_vars_by_name[1]\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name]) for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)\n",
    "\n",
    "X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(critic_net * tf.one_hot(X_action, n_output), axis=1, keepdims=True)\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter('logs', tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 100000\n",
    "replay_mem_size = 1000000\n",
    "replay_mem = deque([], maxlen=replay_mem_size)\n",
    "\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min)*step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_output)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "def sample_mem(batch_size):\n",
    "    indices = np.random.permutation(len(replay_mem))[:batch_size]\n",
    "    cols = [[], [], [], [], []]\n",
    "    for idx in indices:\n",
    "        memory = replay_mem[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_human(env):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    player = 0\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        with tf.variable_scope('actor'):\n",
    "            while not done:\n",
    "                env.render()\n",
    "                step = input()\n",
    "                obs, reward, done, info = env.step('{}{}'.format(player%2+1, step))\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                player += 1\n",
    "                state = convert_game_to_x_state(obs)\n",
    "                action = np.argmax(actor_net.eval(feed_dict={X_state: [state]}))\n",
    "                obs, reward, done, info = env.step(convert_action_to_step(action, player%2+1))\n",
    "\n",
    "                player += 1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000000\n",
    "training_start = 1000\n",
    "training_interval = 3\n",
    "save_steps = 1000\n",
    "copy_steps = 100\n",
    "print_steps = 1000\n",
    "discount_rate = 0.95\n",
    "batch_size = 50\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn_tictactoe_h100_lr0.01.ckpt\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "done = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if path.exists(checkpoint_path + '.meta'):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    \n",
    "    s_time = timer.timer()\n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        \n",
    "        iteration += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = convert_game_to_x_state(obs)\n",
    "            player = 0\n",
    "        \n",
    "        q_values = actor_net.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "        \n",
    "        # Actor plays\n",
    "        obs, reward, done, info = env.step(convert_action_to_step(action, player+1))\n",
    "        next_state = convert_game_to_x_state(obs)\n",
    "        player = (player+1) % 2\n",
    "        \n",
    "        # memorize\n",
    "        replay_mem.append((state, action, reward, next_state, 1.0-done))\n",
    "        state = next_state\n",
    "        \n",
    "        if iteration % print_steps == 0:\n",
    "            e_time = timer.timer()\n",
    "            print('Steps:{}'.format(step), 'Time:{0:.2f}s'.format(e_time-s_time))\n",
    "            s_time = e_time\n",
    "        \n",
    "        if iteration % training_interval != 0:\n",
    "            continue\n",
    "        \n",
    "        # Critic learns\n",
    "        X_state_val, X_action_val, rewards, X_next_state, continues = (sample_mem(batch_size))\n",
    "        next_q_values = actor_net.eval(feed_dict={X_state: X_next_state})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        training_op.run(feed_dict={X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "        \n",
    "        if step % copy_steps:\n",
    "            copy_critic_to_actor.run()\n",
    "            \n",
    "        if step % save_steps:\n",
    "            saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn_tictactoe_h100_lr0.01.ckpt\n",
      "- - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "000\n",
      "x - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "- - o    - - -    - - -    \n",
      "011\n",
      "x - -    - - -    - - o    \n",
      "- x -    - - -    - - -    \n",
      "- - o    - - -    - - -    \n",
      "112\n",
      "x - -    - - -    - - o    \n",
      "- x -    - - -    - - -    \n",
      "- - o    - x -    - - o    \n",
      "012\n",
      "x - -    - - -    - - o    \n",
      "- x -    - - -    - - -    \n",
      "- x o    - x -    - - o    \n"
     ]
    }
   ],
   "source": [
    "play_with_human(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    print(global_step.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x - -    - - -    - - o    \n",
      "- x -    - - -    - - -    \n",
      "- x o    - x -    - - o    \n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
