{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "import gym\n",
    "import gym_tictactoe\n",
    "from timeit import Timer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "n_input = (3 * 3 * 3) * 3\n",
    "n_hidden = 200\n",
    "n_output = 3 * 3 * 3\n",
    "learning_rate = 0.01\n",
    "\n",
    "checkpoint_path = './my_dqn_tictactoe_v2_h{}_lr{}.ckpt'.format(n_hidden, learning_rate)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "env = gym.make('tictactoe-v0')\n",
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(np.array([[0], [1], [2]]))\n",
    "\n",
    "def convert_game_to_x_state(obs):\n",
    "    # gym_tictactoe now supports int-encoded world\n",
    "    world = np.array(obs, dtype=np.float32)\n",
    "    data = list(map(lambda x: [x], world.flatten()))\n",
    "    return encoder.transform(data).flatten()\n",
    "\n",
    "def convert_action_to_step(action, player):\n",
    "    action = int(action)\n",
    "    val = 0\n",
    "    multiplier = 1\n",
    "    while action:\n",
    "        val += (action%3)*multiplier\n",
    "        multiplier *= 10\n",
    "        action //= 3\n",
    "    \n",
    "    return str(player) + str(val).zfill(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_scopes = ['actor', 'critic']\n",
    "all_logits = []\n",
    "outputs = []\n",
    "ys = []\n",
    "all_network_trainable_vars_by_name = []\n",
    "\n",
    "X_state = tf.placeholder(shape=(None, n_input), dtype=tf.float32)\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "for scope in player_scopes:\n",
    "    with tf.variable_scope(scope) as tf_scope:\n",
    "        hidden = fully_connected(X_state, n_hidden, activation_fn=tf.nn.relu, weights_initializer=initializer)\n",
    "        logits = fully_connected(hidden, n_output, activation_fn=None, weights_initializer=initializer)\n",
    "        all_logits.append(logits)\n",
    "\n",
    "        output = tf.contrib.layers.softmax(logits)\n",
    "        outputs.append(output)\n",
    "        \n",
    "        y = tf.to_float(tf.multinomial(tf.log(output), num_samples=n_output))\n",
    "        ys.append(y)\n",
    "        \n",
    "        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "        network_trainable_vars_by_name = { var.name[len(tf_scope.name):]: var for var in trainable_vars }\n",
    "        all_network_trainable_vars_by_name.append(network_trainable_vars_by_name)\n",
    "\n",
    "actor_net = outputs[0]\n",
    "critic_net = outputs[1]\n",
    "\n",
    "actor_vars = all_network_trainable_vars_by_name[0]\n",
    "critic_vars = all_network_trainable_vars_by_name[1]\n",
    "\n",
    "copy_ops = [actor_var.assign(critic_vars[var_name]) for var_name, actor_var in actor_vars.items()]\n",
    "copy_critic_to_actor = tf.group(*copy_ops)\n",
    "\n",
    "X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "q_value = tf.reduce_sum(critic_net * tf.one_hot(X_action, n_output), axis=1, keepdims=True)\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "file_writer = tf.summary.FileWriter('logs', tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "eps_min = 0.05\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 100000\n",
    "replay_mem_size = 1000000\n",
    "replay_mem = deque([], maxlen=replay_mem_size)\n",
    "\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min)*step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_output)\n",
    "    else:\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "def sample_mem(batch_size):\n",
    "    indices = np.random.permutation(len(replay_mem))[:batch_size]\n",
    "    cols = [[], [], [], [], []]\n",
    "    for idx in indices:\n",
    "        memory = replay_mem[idx]\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_with_human(env, human_starts=True):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    player = 0\n",
    "    human_starts_round = human_starts\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        with tf.variable_scope('actor'):\n",
    "            while not done:\n",
    "                if human_starts_round:\n",
    "                    env.render()\n",
    "                    step = input()\n",
    "                    obs, reward, done, info = env.step('{}{}'.format(player%2+1, step))\n",
    "                    player += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                human_starts_round=True\n",
    "                state = convert_game_to_x_state(obs)\n",
    "                action = np.argmax(actor_net.eval(feed_dict={X_state: [state]}))\n",
    "                obs, reward, done, info = env.step(convert_action_to_step(action, player%2+1))\n",
    "\n",
    "                player += 1\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000000\n",
    "training_start = 1000\n",
    "training_interval = 3\n",
    "save_steps = 1000\n",
    "copy_steps = 100\n",
    "print_steps = 1000\n",
    "discount_rate = 0.95\n",
    "batch_size = 50\n",
    "done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn_tictactoe_v2_h200_lr0.01.ckpt\n",
      "74945\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    print(global_step.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_dqn_tictactoe_v2_h200_lr0.01.ckpt\n",
      "- - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "011\n",
      "- - -    - - -    o - -    \n",
      "- x -    - - -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "111\n",
      "- - -    - o -    o - -    \n",
      "- x -    - x -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "010\n",
      "o x -    - o -    o - -    \n",
      "- x -    - x -    - - -    \n",
      "- - -    - - -    - - -    \n",
      "022\n",
      "O x -    O o -    O - -    \n",
      "- x -    - x -    - - -    \n",
      "- - x    - - -    - - -    \n"
     ]
    }
   ],
   "source": [
    "play_with_human(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - x    - - -    o - -    \n",
      "- x -    - x -    - - -    \n",
      "- - o    - - -    - - -    \n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
